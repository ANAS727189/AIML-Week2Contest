{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91407633",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-13T12:47:54.477521Z",
     "iopub.status.busy": "2023-12-13T12:47:54.477053Z",
     "iopub.status.idle": "2023-12-13T12:47:57.025788Z",
     "shell.execute_reply": "2023-12-13T12:47:57.024585Z"
    },
    "papermill": {
     "duration": 2.557076,
     "end_time": "2023-12-13T12:47:57.028810",
     "exception": false,
     "start_time": "2023-12-13T12:47:54.471734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/aimprove-contest-week-2/Test-set-AImprove-2- detailed.csv\n",
      "/kaggle/input/aimprove-contest-week-2/Test-set-AImprove-2..csv\n",
      "/kaggle/input/aimprove-contest-week-2/Train-set-AImprove2.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b703c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T12:47:57.036259Z",
     "iopub.status.busy": "2023-12-13T12:47:57.035620Z",
     "iopub.status.idle": "2023-12-13T13:01:09.873454Z",
     "shell.execute_reply": "2023-12-13T13:01:09.871994Z"
    },
    "papermill": {
     "duration": 792.844691,
     "end_time": "2023-12-13T13:01:09.876347",
     "exception": false,
     "start_time": "2023-12-13T12:47:57.031656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for RandomForest: {'RandomForest__max_depth': 20, 'RandomForest__min_samples_split': 3, 'RandomForest__n_estimators': 400}\n",
      "Best MSE for RandomForest: 0.1067415899615745\n",
      "Best Parameters for GradientBoosting: {'GradientBoosting__learning_rate': 0.05, 'GradientBoosting__max_depth': 3, 'GradientBoosting__n_estimators': 100}\n",
      "Best MSE for GradientBoosting: 0.10684482165255978\n",
      "Best Parameters for DecisionTree: {'DecisionTree__max_depth': 20, 'DecisionTree__min_samples_split': 4}\n",
      "Best MSE for DecisionTree: 0.15619275960765844\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "train_data = pd.read_csv('/kaggle/input/aimprove-contest-week-2/Train-set-AImprove2.csv')\n",
    "test_data = pd.read_csv('/kaggle/input/aimprove-contest-week-2/Test-set-AImprove-2- detailed.csv')\n",
    "\n",
    "# Fill missing values in 'Events' column with 'No Event Occurred'\n",
    "train_data['Events'].fillna('No Event Occurred', inplace=True)\n",
    "\n",
    "# Replace non-numeric values with NaN in 'PrecipitationSumInches' column\n",
    "train_data['PrecipitationSumInches'] = pd.to_numeric(train_data['PrecipitationSumInches'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in the target column\n",
    "train_data.dropna(subset=['PrecipitationSumInches'], inplace=True)\n",
    "\n",
    "X_train = train_data.drop(columns=['PrecipitationSumInches', 'Date'])\n",
    "y_train = train_data['PrecipitationSumInches']\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Preprocessing using ColumnTransformer\n",
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Models\n",
    "models = [\n",
    "    ('RandomForest', RandomForestRegressor(random_state=0)),\n",
    "    ('GradientBoosting', GradientBoostingRegressor(random_state=0)),\n",
    "    ('DecisionTree', DecisionTreeRegressor(random_state=0))\n",
    "]\n",
    "\n",
    "# Define hyperparameters grid for each model\n",
    "params = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [200, 300, 400],\n",
    "        'max_depth': [20, 25, 30],\n",
    "        'min_samples_split': [2, 3, 4]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [100, 150, 200],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'learning_rate': [0.05, 0.1, 0.2]\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'max_depth': [15, 20, 25],\n",
    "        'min_samples_split': [2, 3, 4]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Combine preprocessing with models in a pipeline\n",
    "best_models = {}\n",
    "\n",
    "for model_name, model in models:\n",
    "    param_grid = {f'{model_name}__{key}': value for key, value in params[model_name].items()}\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), (model_name, model)])\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best Parameters for {model_name}:\", grid_search.best_params_)\n",
    "    print(f\"Best MSE for {model_name}:\", -grid_search.best_score_)\n",
    "    \n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "\n",
    "# Further Model Tuning or Ensemble Methods can be applied here to improve performance\n",
    "# You can also add more models and explore different hyperparameters\n",
    "\n",
    "# Process test data\n",
    "test_data['Events'] = train_data['Events']  # Use the same 'Events' column from the training set\n",
    "X_test_processed = test_data.drop(columns=['Date'], errors='ignore')\n",
    "\n",
    "# Predict on the test set using the best models\n",
    "predictions = {}\n",
    "for model_name, model in best_models.items():\n",
    "    test_predictions = model.predict(X_test_processed)\n",
    "    predictions[model_name] = test_predictions\n",
    "\n",
    "# Create submission DataFrames and save to CSV\n",
    "for model_name, test_predictions in predictions.items():\n",
    "    submission_df = pd.DataFrame({\n",
    "        'Date': test_data['Date'],\n",
    "        f'{model_name}_Predictions': test_predictions\n",
    "    })\n",
    "    \n",
    "    submission_df = submission_df.drop_duplicates(subset=['Date']).dropna(subset=['Date'])\n",
    "    \n",
    "    submission_df.to_csv(f'/kaggle/working/{model_name}_Test_Predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7258022,
     "sourceId": 65652,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30615,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 800.16828,
   "end_time": "2023-12-13T13:01:10.705491",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-13T12:47:50.537211",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
